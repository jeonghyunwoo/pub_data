{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from newspaper import Article\n",
    "from time import sleep\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.검색어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '가계부채'\n",
    "qry1 = quote_plus(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url0 = \"https://search.naver.com/search.naver?&where=news&query={}&start={}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가계부채 search done: 00:07:38 elapsed\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "title = []\n",
    "href = []\n",
    "date = []\n",
    "text = []\n",
    "stime = time.time()\n",
    "# original 주소 \n",
    "url0 = \"https://search.naver.com/search.naver?&where=news&query={}&start={}\"\n",
    "# 네이버 검색결과 page별\n",
    "for i in range(3): \n",
    "    start = i*10+1\n",
    "    url = url0.format(qry1,start)\n",
    "    obj = soup(urlopen(url),'html.parser')\n",
    "    # links : naver 검색결과 뉴스들의 링크들\n",
    "    links = obj.find_all('a',{'class':'_sp_each_title'})\n",
    "    # 링크별 기사가져오기 \n",
    "    for link in links:\n",
    "        h = link.get('href') # hyperlink\n",
    "        a = Article(h,language='ko') # 기사가져오기 \n",
    "        a.download();a.parse()\n",
    "        articles.append(a)\n",
    "        title.append(a.title)\n",
    "        date.append(a.publish_date)\n",
    "        text.append(a.text)\n",
    "        sleep(np.random.randint(3,30))\n",
    "etime = time.time() - stime\n",
    "elaps = time.strftime('%H:%M:%S',time.gmtime(etime))\n",
    "news = pd.DataFrame(dict(qry=qry,date=date,title=title,text=text))        \n",
    "print('{} search done: {} elapsed'.format(qry,elaps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**naver_search** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naver news 검색 \n",
    "def naver_search(qry,page=3):\n",
    "    articles = []\n",
    "    title = []\n",
    "    href = []\n",
    "    date = []\n",
    "    text = []\n",
    "    stime = time.time()\n",
    "    # original 주소 \n",
    "    url0 = \"https://search.naver.com/search.naver?where=news&query={}&ds={}&de={}\"\n",
    "    url0 = \"https://search.naver.com/search.naver?&where=news&query={}&start={}\"\n",
    "    # 네이버 검색결과 page별\n",
    "    for i in range(page): \n",
    "        start = i*10+1\n",
    "        qry1 = quote_plus(qry)\n",
    "        url = url0.format(qry1,start)\n",
    "        obj = soup(urlopen(url),'html.parser')\n",
    "        # links : naver 검색결과 뉴스들의 링크들\n",
    "        links = obj.find_all('a',{'class':'_sp_each_title'})\n",
    "        # 링크별 기사가져오기 \n",
    "        for link in links:\n",
    "            h = link.get('href') # hyperlink\n",
    "            a = Article(h,language='ko') # 기사가져오기 \n",
    "            a.download();a.parse()\n",
    "            articles.append(a)\n",
    "            title.append(a.title)\n",
    "            date.append(a.publish_date)\n",
    "            text.append(a.text)\n",
    "            sleep(np.random.randint(1,20))\n",
    "    etime = time.time() - stime\n",
    "    elaps = time.strftime('%H:%M:%S',time.gmtime(etime))\n",
    "    news = pd.DataFrame(dict(qry=qry,date=date,title=title,text=text))        \n",
    "    print('{} search done: {} elapsed'.format(qry,elaps))\n",
    "    return news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**naver_search_d** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20100101'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beg = '2010.01.01'\n",
    "beg.replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naver news 기간검색\n",
    "def naver_search_d(qry,beg,end,pages=3): \n",
    "    articles = []\n",
    "    title = []\n",
    "    href = []\n",
    "    date = []\n",
    "    text = []\n",
    "    stime = time.time()\n",
    "    # original 주소 \n",
    "    url0 = \"https://search.naver.com/search.naver?&where=news&query={0}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={2}&docid=&nso=so:r,p:from{3}to{4},a:all&mynews=0&cluster_rank=22&start={5}&refresh_start=0\"\n",
    "    # 네이버 검색결과 page별\n",
    "    begx = beg.replace('.','')\n",
    "    endx = end.replace('.','')\n",
    "    for i in range(pages): \n",
    "        start = i*10+1\n",
    "        qry1 = quote_plus(qry)\n",
    "        url = url0.format(qry1,beg,end,begx,endx,start)\n",
    "        obj = soup(urlopen(url),'html.parser')\n",
    "        # links : naver 검색결과 뉴스들의 링크들\n",
    "        links = obj.find_all('a',{'class':'_sp_each_title'})\n",
    "        # 링크별 기사가져오기 \n",
    "        for link in links:\n",
    "            h = link.get('href') # hyperlink\n",
    "            try:\n",
    "                a = Article(h,language='ko') # 기사가져오기 \n",
    "                a.download();a.parse()\n",
    "                articles.append(a)\n",
    "                title.append(a.title)\n",
    "                date.append(a.publish_date)\n",
    "                text.append(a.text)\n",
    "            except:\n",
    "                pass\n",
    "            sleep(np.random.randint(1,7))\n",
    "    etime = time.time() - stime\n",
    "    elaps = time.strftime('%H:%M:%S',time.gmtime(etime))\n",
    "    news = pd.DataFrame(dict(qry=qry,date=date,title=title,text=text))        \n",
    "    print('{} search done: {} elapsed'.format(qry,elaps))\n",
    "    return news\n",
    "# naver_search_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news2010 = naver_search_d('가계부채','2010.01.01','2010.12.31',pages=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qry</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>가계부채</td>\n",
       "      <td>2010-12-29 12:00:00+09:00</td>\n",
       "      <td>재정부 \"가계부채 관리가능한 수준\"</td>\n",
       "      <td>기획재정부는 29일 가계부채 수준이 비교적 높지만 가계부채가 고소득층에 집중돼 있는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>가계부채</td>\n",
       "      <td>None</td>\n",
       "      <td>부채가구 92% \"1년 후 상환능력 있어\"…가계부채 인식 양호</td>\n",
       "      <td>[뉴시스 이시간 핫 뉴스]\\n\\n【서울=뉴시스】류난영 기자 = 우리나라의 부채가구가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>가계부채</td>\n",
       "      <td>2010-12-23 09:23:52+09:00</td>\n",
       "      <td>김종창 \"가계부채, PF대출, 외국자본 유출입에 능동적 대응\"</td>\n",
       "      <td>김종창 금융감독원장은 23일 \"앞으로도 가계부채, PF대출, 외국자본의 유출입 등 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>가계부채</td>\n",
       "      <td>2010-12-22 13:28:51</td>\n",
       "      <td>진동수 \"가계부채, 문제는 증가 속도\"</td>\n",
       "      <td>박민규 기자 yushin@\\n\\n\\n\\n\\n\\n[아시아경제 박민규 기자] 진동수 금...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>가계부채</td>\n",
       "      <td>None</td>\n",
       "      <td>기업銀 '부채 클리닉 서비스', \"가계 빚 고민 상담 해드려요\"</td>\n",
       "      <td>한국아이닷컴 인기기사\\n\\n기업은행이 개인 고객들의 가계부채 해결 도우미로 나선다....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qry                       date                                title  \\\n",
       "4  가계부채  2010-12-29 12:00:00+09:00                  재정부 \"가계부채 관리가능한 수준\"   \n",
       "5  가계부채                       None   부채가구 92% \"1년 후 상환능력 있어\"…가계부채 인식 양호   \n",
       "6  가계부채  2010-12-23 09:23:52+09:00   김종창 \"가계부채, PF대출, 외국자본 유출입에 능동적 대응\"   \n",
       "7  가계부채        2010-12-22 13:28:51                진동수 \"가계부채, 문제는 증가 속도\"   \n",
       "8  가계부채                       None  기업銀 '부채 클리닉 서비스', \"가계 빚 고민 상담 해드려요\"   \n",
       "\n",
       "                                                text  \n",
       "4  기획재정부는 29일 가계부채 수준이 비교적 높지만 가계부채가 고소득층에 집중돼 있는...  \n",
       "5  [뉴시스 이시간 핫 뉴스]\\n\\n【서울=뉴시스】류난영 기자 = 우리나라의 부채가구가...  \n",
       "6  김종창 금융감독원장은 23일 \"앞으로도 가계부채, PF대출, 외국자본의 유출입 등 ...  \n",
       "7  박민규 기자 yushin@\\n\\n\\n\\n\\n\\n[아시아경제 박민규 기자] 진동수 금...  \n",
       "8  한국아이닷컴 인기기사\\n\\n기업은행이 개인 고객들의 가계부채 해결 도우미로 나선다....  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news2010.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "frm = pd.date_range('2010-01','2018-01',freq='YS').strftime('%Y.%m.%d')\n",
    "tom = pd.date_range('2010-12','2019-12',freq='Y').strftime('%Y.%m.%d')\n",
    "news = []\n",
    "for f,t in zip(frm,tom):\n",
    "    news.append(naver_search_d('가계부채',f,t,page=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdf = pd.concat(news,axis=0)\n",
    "newsdf.to_csv('o:/보낼파일함/news.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
